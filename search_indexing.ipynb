{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.qparser import *\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED, NUMERIC\n",
    "from whoosh.analysis import StemmingAnalyzer, StandardAnalyzer\n",
    "from whoosh import index\n",
    "from whoosh.index import create_in\n",
    "import os, os.path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1 = Schema(author=TEXT(analyzer=StandardAnalyzer(stoplist=None), stored=True),\n",
    "                 day=NUMERIC(stored=False),\n",
    "                 id=TEXT(analyzer=StandardAnalyzer(stoplist=None), stored=False),\n",
    "                 link=TEXT(analyzer=StandardAnalyzer(stoplist=None), stored=True),\n",
    "                 month=NUMERIC(stored=False),\n",
    "                 abstract=TEXT(analyzer=StandardAnalyzer(stoplist=None), stored=True),\n",
    "                 tag=TEXT(analyzer=StandardAnalyzer(stoplist=None), stored=False),\n",
    "                 title=TEXT(analyzer=StandardAnalyzer(stoplist=None), stored=True),\n",
    "                 year=NUMERIC(stored=True)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1\n"
    }
   ],
   "source": [
    "file_dir = '/Users/tony/Desktop/search_engine/filedir/'\n",
    "file_name = os.listdir(file_dir)\n",
    "print(len(file_name))\n",
    "\n",
    "# reading files \n",
    "file_load = json.load(open('/Users/tony/Desktop/search_engine/filedir/arxivData.json', 'r'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'str'>\n"
    }
   ],
   "source": [
    "\n",
    "print(type(file_load[0]['author']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 files loaded\n1000 files loaded\n2000 files loaded\n3000 files loaded\n4000 files loaded\n5000 files loaded\n6000 files loaded\n7000 files loaded\n8000 files loaded\n9000 files loaded\n10000 files loaded\n11000 files loaded\n12000 files loaded\n13000 files loaded\n14000 files loaded\n15000 files loaded\n16000 files loaded\n17000 files loaded\n18000 files loaded\n19000 files loaded\n20000 files loaded\n21000 files loaded\n22000 files loaded\n23000 files loaded\n24000 files loaded\n25000 files loaded\n26000 files loaded\n27000 files loaded\n28000 files loaded\n29000 files loaded\n30000 files loaded\n31000 files loaded\n32000 files loaded\n33000 files loaded\n34000 files loaded\n35000 files loaded\n36000 files loaded\n37000 files loaded\n38000 files loaded\n39000 files loaded\n40000 files loaded\n"
    }
   ],
   "source": [
    "#to create an index in a dictionary\n",
    "if not os.path.exists(\"indexdir\"):\n",
    "    os.mkdir(\"indexdir\")\n",
    "ix = index.create_in(\"indexdir\", schema1)\n",
    "#open an existing index object\n",
    "ix = index.open_dir(\"indexdir\")\n",
    "#create a writer object to add documents to the index\n",
    "writer = ix.writer()\n",
    "#now we can add documents to the index\n",
    "\n",
    "for i in range(len(file_load)):\n",
    "    if (i%1000) == 0: \n",
    "        print(\"%s files loaded\" % (i))\n",
    "\n",
    "    writer.add_document(author=file_load[i]['author'],\n",
    "                        day=file_load[i]['day'],\n",
    "                        id=file_load[i]['id'],\n",
    "                        link=file_load[i]['link'],\n",
    "                        month=file_load[i]['month'],\n",
    "                        abstract=file_load[i]['summary'],\n",
    "                        tag=file_load[i]['tag'],\n",
    "                        title=file_load[i]['title'],\n",
    "                        year=file_load[i]['year'])\n",
    "\n",
    "#close the writer and save the added documents in the index\n",
    "#you should call the commit() function once you finish adding the documents otherwise you will cause an error-\n",
    "#when you try to edit the index next time and open another writer. \n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(abstract:apple AND abstract:company AND abstract:department)\n"
    }
   ],
   "source": [
    "#parsing the query\n",
    "# this is just a simple parser with default field\n",
    "parser=QueryParser(\"abstract\",schema=schema1) \n",
    "#if you want “unfielded” terms to search both the title and content fields,  use a whoosh.qparser.MultifieldParser\n",
    "#parser = MultifieldParser([\"title\", \"abstract\"], schema=schema)\n",
    "#call parse() on query to parse a query string into a query object\n",
    "result=parser.parse(u\"apple company department\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "author:young~\n2D-3D Pose Consistency-based Conditional Random Fields for 3D Human Pose\n  Estimation\nname': 'Ju <b class=\"match term0\">Yong</b> Chang'}, {'name': '<b class=\"match term1\">Kyoung</b> Mu Lee\nV2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and\n  Human Pose Estimation from a Single Depth Map\nMoon'}, {'name': 'Ju <b class=\"match term0\">Yong</b> Chang'}, {'name': '<b class=\"match term1\">Kyoung</b> Mu Lee\nHolistic Planimetric prediction to Local Volumetric prediction for 3D\n  Human Pose Estimation\nMoon'}, {'name': 'Ju <b class=\"match term0\">Yong</b> Chang'}, {'name': 'Yumin Suh...name': '<b class=\"match term1\">Kyoung</b> Mu Lee\nHadamard Product for Low-rank Bilinear Pooling\nHwa Kim'}, {'name': '<b class=\"match term1\">Kyoung</b>-Woon On'}, {'name': 'Woosang...Woo Ha'}, {'name': '<b class=\"match term2\">Byoung</b>-Tak Zhang\nThe Evolution of Neural Network-Based Chart Patterns: A Preliminary\n  Study\nname': '<b class=\"match term3\">Myoung</b> Hoon Ha'}, {'name': 'Byung\nMultiscale Hybrid Non-local Means Filtering Using Modified Similarity\n  Measure\nname': 'Dai-<b class=\"match term4\">Gyoung</b> Kim\nUsing social network graph analysis for interest detection\nname': 'Brian Lee <b class=\"match term0\">Yung</b> Rowe\nWays of Conditioning Generative Adversarial Networks\nKwak'}, {'name': '<b class=\"match term2\">Byoung</b>-Tak Zhang\nGenerating Images Part by Part with Composite Generative Adversarial\n  Networks\nKwak'}, {'name': '<b class=\"match term2\">Byoung</b>-Tak Zhang\nJoint Estimation of Camera Pose, Depth, Deblurring, and Super-Resolution\n  from a Blurred Image Sequence\nPark'}, {'name': '<b class=\"match term1\">Kyoung</b> Mu Lee\n"
    }
   ],
   "source": [
    "#FuzzyTermPlugin lets you search for “fuzzy” terms, that is, terms that don’t have to match exactly. \n",
    "#The fuzzy term will match any similar term within a certain number of “edits” \n",
    "parser.add_plugin(FuzzyTermPlugin())\n",
    "result=parser.parse(u\"author:Young~\")#would match a document has Margare and all terms in the index within one “edit” of cat, for example Margaret insert t\n",
    "print(result)\n",
    "#searcher object is used for searching the matched documents\n",
    "#you can open the searcher using a with statement so the searcher is automatically closed when you’re done with it\n",
    "#ix is the document index we created before\n",
    "with ix.searcher() as searcher:\n",
    "    results=searcher.search(result)#The Results object acts like a list of the matched documents.\n",
    "    '''\n",
    "    for i in range(10):\n",
    "        print (results[i])\n",
    "    '''\n",
    "    for hit in results:\n",
    "        #print(hit[\"title\"])\n",
    "        print(hit.highlights(\"author\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}